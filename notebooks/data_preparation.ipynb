{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "The following cells will improve the DF, which presents inconsistency, missing values and outliers, thanks to consideration done during the data understanding phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zmlinux/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3145: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# load df\n",
    "df = pd.read_csv(\"../dataset/customer_supermarket_understanding.csv\", index_col=0, parse_dates=[\"BasketDate\"], decimal=\",\")\n",
    "df.Sale = df.Sale.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every canceled basket ('C'+'BasketID') check if exists at least one counterpart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = df[(df['BasketID'].str.contains('C')) & (df['ProdID'] != 'D')][['CustomerID','Qta','ProdID']]\n",
    "for index, col in  df_check.iterrows():\n",
    "    if df[(df['CustomerID'] == col[0]) & (df['Qta'] == -col[1]) & (df['ProdID'] == col[2])].shape[0] == 0: \n",
    "        print(True)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove from the dataset the rows with canceled basket and possible counterpart (if there are more then one counterpart, the first will be deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of dropped rows:  11775\n"
     ]
    }
   ],
   "source": [
    "df_canceled_basket = df[(df['BasketID'].str.contains('C')) & (df['ProdID'] != 'D')]\n",
    "\n",
    "rows_with_counterparts = []\n",
    "rows_without_counterparts = []\n",
    "\n",
    "for index, col in df_canceled_basket.iterrows():\n",
    "    df_temp = df[(df['CustomerID'] == col['CustomerID']) & (df['Qta'] == -col['Qta']) & (df['ProdID'] == col['ProdID'])]\n",
    "    \n",
    "    if df_temp.shape[0] == 0: \n",
    "        rows_without_counterparts.append(index)\n",
    "    else:\n",
    "        rows_with_counterparts.append(index)\n",
    "        rows_with_counterparts.append(df_temp.index[0])\n",
    "    \n",
    "'''\n",
    "print(\"DF len before dropping rows with counterparts: \", len(df))\n",
    "df_canceled_basket = df.drop(df.index[rows_with_counterparts])\n",
    "print(\"DF len after deleting rows with counterparts: \", len(df_canceled_basket))\n",
    "\n",
    "df_canceled_basket = df_canceled_basket.drop(df_canceled_basket.index[rows_without_counterparts])\n",
    "print(\"DF len after deleting rows without counterparts: \", len(df_canceled_basket))\n",
    "'''\n",
    "\n",
    "rows_to_be_dropped = rows_with_counterparts + rows_without_counterparts\n",
    "\n",
    "df_canceled_basket = df.drop(df.index[rows_to_be_dropped])\n",
    "\n",
    "print(\"Total number of dropped rows: \", len(df)-len(df_canceled_basket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_canceled_basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inconsistency resolution\n",
    "\n",
    "def inconsistency_resolver(path,col1,col2):\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        inconsistent_list = json.load(f)\n",
    "        \n",
    "    df_inconsistent = df[df[col1].isin(inconsistent_list)]\n",
    "\n",
    "    df_grouped = df_inconsistent.groupby([col1,col2]).size().reset_index()\n",
    "    \n",
    "    df_grouped = df_grouped.sort_values(0, ascending=False).drop_duplicates(col1).sort_index()\n",
    "    \n",
    "    mydict = pd.Series(df_grouped[col2].values,index=df_grouped[col1]).to_dict()\n",
    "    \n",
    "    for k,v in mydict.items():\n",
    "        \n",
    "        df.loc[df[col1] == k, col2] = v\n",
    "\n",
    "inconsistency_resolver(\"../dataset/inconsistent_CustomerID_CustomerCountry.json\",\"CustomerID\",\"CustomerCountry\") \n",
    "inconsistency_resolver(\"../dataset/inconsistent_ProdID_ProdDescr.json\",\"ProdID\",\"ProdDescr\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' check if inconsistency has been solved\n",
    "def inconsistent_set(K,V):\n",
    "    \n",
    "    inconsistentset = list()\n",
    "\n",
    "    for key in df[K].unique().tolist():\n",
    "        temp_df = df[df[K] == key]\n",
    "        valueslist = temp_df[V].tolist()\n",
    "        for value in valueslist:\n",
    "            if(valueslist[0] != value):\n",
    "                inconsistentset.append(key)\n",
    "                break;\n",
    "    return inconsistentset\n",
    "\n",
    "\n",
    "# 1\n",
    "ProdID_ProdDescr_IS = inconsistent_set(\"ProdID\",\"ProdDescr\")\n",
    "            \n",
    "print(\"Number of not consistent ProdDescr:\", len(ProdID_ProdDescr_IS))\n",
    "\n",
    "#3 \n",
    "CustomerID_CustomerCountry_IS = inconsistent_set(\"CustomerID\",\"CustomerCountry\")\n",
    "            \n",
    "print(\"Number of not consistent CustomerCountry:\", len(CustomerID_CustomerCountry_IS))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF without inconsistency serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../dataset/customer_supermarket_no_inconsistency.csv\", sep=\"\\t\", decimal=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   BasketID          BasketDate  ProdID  Qta  Sale  CustomerID  \\\n",
      "0    536365 2010-01-12 08:26:00   21730    6  4.25       17850   \n",
      "1    536365 2010-01-12 08:26:00   22752    2  7.65       17850   \n",
      "2    536365 2010-01-12 08:26:00   71053    6  3.39       17850   \n",
      "3    536365 2010-01-12 08:26:00  84029E    6  3.39       17850   \n",
      "4    536365 2010-01-12 08:26:00  84029G    6  3.39       17850   \n",
      "\n",
      "  CustomerCountry                            ProdDescr  Outlier  \n",
      "0  United Kingdom    GLASS STAR FROSTED T-LIGHT HOLDER    False  \n",
      "1  United Kingdom         SET 7 BABUSHKA NESTING BOXES    False  \n",
      "2  United Kingdom                  WHITE METAL LANTERN    False  \n",
      "3  United Kingdom       RED WOOLLY HOTTIE WHITE HEART.    False  \n",
      "4  United Kingdom  KNITTED UNION FLAG HOT WATER BOTTLE    False  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 387641 entries, 0 to 387880\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   BasketID         387641 non-null  int64         \n",
      " 1   BasketDate       387641 non-null  datetime64[ns]\n",
      " 2   ProdID           387641 non-null  object        \n",
      " 3   Qta              387641 non-null  int64         \n",
      " 4   Sale             387641 non-null  float64       \n",
      " 5   CustomerID       387641 non-null  int64         \n",
      " 6   CustomerCountry  387641 non-null  object        \n",
      " 7   ProdDescr        387641 non-null  object        \n",
      " 8   Outlier          387641 non-null  bool          \n",
      "dtypes: bool(1), datetime64[ns](1), float64(1), int64(3), object(3)\n",
      "memory usage: 27.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# first of all we deserialize our dataframe\n",
    "df = pd.read_csv(\"../dataset/customer_supermarket_no_inconsistency.csv\", sep=\"\\t\", index_col=0, parse_dates=[\"BasketDate\"], decimal=\",\")\n",
    "# second remove outliers from df\n",
    "df = df[df['Outlier'] == False]\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - indicator\n",
    "the total number of items purchased by a customer during the period of\n",
    "observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = df.groupby(\"CustomerID\").sum().reset_index()\n",
    "df_i = df_i[[\"CustomerID\", \"Qta\"]]\n",
    "df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i.sort_values(by='Qta', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!! look at this, it might be an outlier\n",
    "print(df[df.CustomerID == 14646])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iu - indicator\n",
    "the number of distinct items bought by a customer in the period of\n",
    "observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iu = df.groupby('CustomerID')['ProdID'].nunique().reset_index()\n",
    "df_iu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imax - indicator\n",
    "the maximum number of items purchased by a customer during a\n",
    "shopping session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imax = df.groupby([\"CustomerID\", \"BasketID\"]).Qta.sum()\n",
    "df_imax = df_imax.groupby(level=0).head(1).reset_index()\n",
    "\n",
    "#df_imax = df_imax.max(level=0)\n",
    "\n",
    "df_imax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E - indicator\n",
    "the Shannon entropy on the purchasing behaviour of the customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy helper function \n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from math import log, e\n",
    "import pandas as pd   \n",
    "\n",
    "\"\"\" Usage: pandas_entropy(df['column1']) \"\"\"\n",
    "\n",
    "def pandas_entropy(column, base=None):\n",
    "    vc = pd.Series(column).value_counts(normalize=True, sort=False)\n",
    "    base = e if base is None else base\n",
    "    return -(vc * np.log(vc)/np.log(base)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.groupby([\"CustomerID\", \"BasketID\"]).Qta.sum().reset_index()\n",
    "print(df_temp)\n",
    "\n",
    "\n",
    "for customer in df_temp.CustomerID.unique():\n",
    "    customer_baskets = df_temp[df_temp.CustomerID == customer]\n",
    "    print(pandas_entropy(customer_baskets[\"Qta\"]))\n",
    "    #print(df_temp[df_temp.CustomerID == customer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting together all indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_entropy, df_imax.Qta, df_iu.ProdID, df_i.Qta]\n",
    "indicators = pd.concat(frames, join='outer', axis=1)\n",
    "indicators.columns = (\"CustomerID\", \"Entropy\", \"imax\", \"iu\", \"i\")\n",
    "print(indicators.head())\n",
    "\n",
    "indicators.to_csv(\"../dataset/indicators.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer Spending Profile \n",
    "we categorize each customer as either low, medium, or high spending according to their average expense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profile = df.groupby([\"CustomerID\"]).agg({\"Sale\":sum, \"Qta\":sum})\n",
    "\n",
    "binwidth = 50\n",
    "bins=range(0, 1000 + binwidth, binwidth)\n",
    "print(bins)\n",
    "n, bins, patches = plt.hist(df_profile.Sale, bins=bins, facecolor='blue', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "spending_profile = pd.cut(df_profile['Sale'], bins=[0, 100, 300, df_profile.Sale.max()], include_lowest=True, labels=[\"low\", \"medium\", \"high\"])\n",
    "spending_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average cost of a basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_basket_groupby_sum = df.groupby([\"CustomerID\",'BasketID'], as_index=False).agg({\"Sale\":sum})\n",
    "\n",
    "series_customer_basket_groupby_mean = df_customer_basket_groupby_sum.groupby('CustomerID')['Sale'].mean()\n",
    "df_customer_basket_groupby_mean = pd.DataFrame(df_customer_basket_groupby_mean)\n",
    "\n",
    "binwidth = 50\n",
    "bins=range(0, 400 + binwidth, binwidth)\n",
    "print(bins)\n",
    "n, bins, patches = plt.hist(df_customer_basket_groupby_mean.Sale, bins=bins, facecolor='blue', alpha=0.5)\n",
    "plt.savefig('../output/total_receipt_price_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "basket_cost_profile = pd.cut(df_customer_basket_groupby_mean['Sale'], bins=[0, 50, 200, df_customer_basket_groupby_mean.Sale.max()], include_lowest=True, labels=[\"low\", \"medium\", \"high\"])\n",
    "basket_cost_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales per country\n",
    "(is this even an indicator?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_per_country = df.groupby([\"CustomerCountry\"])[\"Sale\"].sum().reset_index()\n",
    "print(df_sales_per_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pie plot is horrible because of england, that probably has some outliers\n",
    "#explode=np.zeros(len(df_sales_per_country.CustomerCountry))\n",
    "\n",
    "plt.pie(df_sales_per_country.Sale, labels=df_sales_per_country.CustomerCountry, autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most bought items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_dropped = [2, 3, 6]\n",
    "df_most_bought = df.groupby([\"ProdID\", \"ProdDescr\"]).sum().reset_index()\n",
    "#df_most_bought = df_most_bought.drop(df_most_bought.columns[cols_dropped], axis=1)\n",
    "df_most_bought = df_most_bought.sort_values(by=\"Qta\", ascending=False)\n",
    "#df_most_bought = df_most_bought.groupby(level=0).head(1).reset_index()\n",
    "df_most_bought \n",
    "\n",
    "#print(df[df.ProdDescr == \"Discount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df_most_bought[:10].Qta, labels=df_most_bought[:10].ProdDescr, autopct='%1.1f%%')\n",
    "plt.savefig(\"../output/most_bought_item_piechart.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most bought item per country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols_dropped = [3, 4, 7]\n",
    "df_mb_country = df.groupby([\"ProdID\", \"ProdDescr\", \"CustomerCountry\"]).sum().reset_index()\n",
    "df_mb_country = df_mb_country.drop([\"Sale\"], axis = 1)\n",
    "\n",
    "df_mb_country = df_mb_country[df_mb_country.groupby([\"CustomerCountry\"])[\"Qta\"].transform(\"max\") == df_mb_country[\"Qta\"]].reset_index()\n",
    "\n",
    "#df_mb_country = df_mb_country.groupby([\"CustomerCountry\"]).agg({\"Qta\" : \"max\"}).reset_index()\n",
    "#print(df_mb_country.CustomerCountry.unique())\n",
    "\n",
    "\n",
    "print(df_mb_country)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end datetimes\n",
    "print(df.BasketDate.min(), df.BasketDate.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly = df.set_index(\"BasketDate\").copy()\n",
    "weekly = weekly.groupby(pd.Grouper(freq='M'))[\"Qta\"].sum()\n",
    "\n",
    "#print(weekly)\n",
    "\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.plot(weekly.index, weekly, color='tab:blue', marker=\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
