{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "The following cells will improve the DF, which presents inconsistency, missing values and outliers, thanks to consideration done during the data understanding phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zmlinux/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3145: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# load df\n",
    "df = pd.read_csv(\"../dataset/customer_supermarket_understanding.csv\", index_col=0, parse_dates=[\"BasketDate\"], decimal=\",\")\n",
    "df.Sale = df.Sale.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every canceled basket ('C'+'BasketID') check if exists at least one counterpart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = df[(df['BasketID'].str.contains('C')) & (df['ProdID'] != 'D')][['CustomerID','Qta','ProdID']]\n",
    "for index, col in  df_check.iterrows():\n",
    "    if df[(df['CustomerID'] == col[0]) & (df['Qta'] == -col[1]) & (df['ProdID'] == col[2])].shape[0] == 0: \n",
    "        print(True)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove from the dataset the rows with canceled basket and possible counterpart (if there are more then one counterpart, the first will be deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of dropped rows:  11775\n"
     ]
    }
   ],
   "source": [
    "df_canceled_basket = df[(df['BasketID'].str.contains('C')) & (df['ProdID'] != 'D')]\n",
    "\n",
    "rows_with_counterparts = []\n",
    "rows_without_counterparts = []\n",
    "\n",
    "for index, col in df_canceled_basket.iterrows():\n",
    "    df_temp = df[(df['CustomerID'] == col['CustomerID']) & (df['Qta'] == -col['Qta']) & (df['ProdID'] == col['ProdID'])]\n",
    "    \n",
    "    if df_temp.shape[0] == 0: \n",
    "        rows_without_counterparts.append(index)\n",
    "    else:\n",
    "        rows_with_counterparts.append(index)\n",
    "        rows_with_counterparts.append(df_temp.index[0])\n",
    "    \n",
    "'''\n",
    "print(\"DF len before dropping rows with counterparts: \", len(df))\n",
    "df_canceled_basket = df.drop(df.index[rows_with_counterparts])\n",
    "print(\"DF len after deleting rows with counterparts: \", len(df_canceled_basket))\n",
    "\n",
    "df_canceled_basket = df_canceled_basket.drop(df_canceled_basket.index[rows_without_counterparts])\n",
    "print(\"DF len after deleting rows without counterparts: \", len(df_canceled_basket))\n",
    "'''\n",
    "\n",
    "rows_to_be_dropped = rows_with_counterparts + rows_without_counterparts\n",
    "\n",
    "df_canceled_basket = df.drop(df.index[rows_to_be_dropped])\n",
    "\n",
    "print(\"Total number of dropped rows: \", len(df)-len(df_canceled_basket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_canceled_basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inconsistency resolution\n",
    "\n",
    "def inconsistency_resolver(path,col1,col2):\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        inconsistent_list = json.load(f)\n",
    "        \n",
    "    df_inconsistent = df[df[col1].isin(inconsistent_list)]\n",
    "\n",
    "    df_grouped = df_inconsistent.groupby([col1,col2]).size().reset_index()\n",
    "    \n",
    "    df_grouped = df_grouped.sort_values(0, ascending=False).drop_duplicates(col1).sort_index()\n",
    "    \n",
    "    mydict = pd.Series(df_grouped[col2].values,index=df_grouped[col1]).to_dict()\n",
    "    \n",
    "    for k,v in mydict.items():\n",
    "        \n",
    "        df.loc[df[col1] == k, col2] = v\n",
    "\n",
    "inconsistency_resolver(\"../dataset/inconsistent_CustomerID_CustomerCountry.json\",\"CustomerID\",\"CustomerCountry\") \n",
    "inconsistency_resolver(\"../dataset/inconsistent_ProdID_ProdDescr.json\",\"ProdID\",\"ProdDescr\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' check if inconsistency has been solved\n",
    "def inconsistent_set(K,V):\n",
    "    \n",
    "    inconsistentset = list()\n",
    "\n",
    "    for key in df[K].unique().tolist():\n",
    "        temp_df = df[df[K] == key]\n",
    "        valueslist = temp_df[V].tolist()\n",
    "        for value in valueslist:\n",
    "            if(valueslist[0] != value):\n",
    "                inconsistentset.append(key)\n",
    "                break;\n",
    "    return inconsistentset\n",
    "\n",
    "\n",
    "# 1\n",
    "ProdID_ProdDescr_IS = inconsistent_set(\"ProdID\",\"ProdDescr\")\n",
    "            \n",
    "print(\"Number of not consistent ProdDescr:\", len(ProdID_ProdDescr_IS))\n",
    "\n",
    "#3 \n",
    "CustomerID_CustomerCountry_IS = inconsistent_set(\"CustomerID\",\"CustomerCountry\")\n",
    "            \n",
    "print(\"Number of not consistent CustomerCountry:\", len(CustomerID_CustomerCountry_IS))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF without inconsistency serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../dataset/customer_supermarket_no_inconsistency.csv\", sep=\"\\t\", decimal=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first of all we deserialize our dataframe\n",
    "df = pd.read_csv(\"../dataset/customer_supermarket_no_inconsistency.csv\", sep=\"\\t\", index_col=0, parse_dates=[\"BasketDate\"], decimal=\",\")\n",
    "# second remove outliers from df\n",
    "df = df[df['Outlier'] == False]\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - indicator\n",
    "the total number of items purchased by a customer during the period of\n",
    "observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = df.groupby(\"CustomerID\").sum().reset_index()\n",
    "df_i = df_i[[\"CustomerID\", \"Qta\"]]\n",
    "df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i.sort_values(by='Qta', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!! look at this, it might be an outlier\n",
    "print(df[df.CustomerID == 14646])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iu - indicator\n",
    "the number of distinct items bought by a customer in the period of\n",
    "observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iu = df.groupby('CustomerID')['ProdID'].nunique().reset_index()\n",
    "df_iu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imax - indicator\n",
    "the maximum number of items purchased by a customer during a\n",
    "shopping session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imax = df.groupby([\"CustomerID\", \"BasketID\"]).Qta.sum()\n",
    "df_imax = df_imax.groupby(level=0).head(1).reset_index()\n",
    "\n",
    "#df_imax = df_imax.max(level=0)\n",
    "\n",
    "df_imax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E - indicator\n",
    "the Shannon entropy on the purchasing behaviour of the customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.groupby([\"CustomerID\", \"BasketID\"]).Qta.sum().reset_index()\n",
    "values = df_temp[\"Qta\"]\n",
    "df_temp['Entropy'] = -(values*np.log(values))\n",
    "df_entropy = df_temp.groupby('CustomerID')['Entropy'].sum().reset_index()\n",
    "# to remove nan values caused by logs\n",
    "df_entropy['Entropy'] = df_entropy['Entropy'].fillna(0)\n",
    "\n",
    "\n",
    "df_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting together all indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_entropy, df_imax.Qta, df_iu.ProdID, df_i.Qta]\n",
    "indicators = pd.concat(frames, join='outer', axis=1)\n",
    "indicators.columns = (\"CustomerID\", \"Entropy\", \"imax\", \"iu\", \"i\")\n",
    "print(indicators.head())\n",
    "\n",
    "indicators.to_csv(\"../dataset/indicators.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer Spending Profile \n",
    "we categorize each customer as either low, medium, or high spending according to their average expense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profile = df.groupby([\"CustomerID\"]).agg({\"Sale\":sum, \"Qta\":sum})\n",
    "\n",
    "binwidth = 50\n",
    "bins=range(0, 1000 + binwidth, binwidth)\n",
    "print(bins)\n",
    "n, bins, patches = plt.hist(df_profile.Sale, bins=bins, facecolor='blue', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "spending_profile = pd.cut(df_profile['Sale'], bins=[0, 100, 300, df_profile.Sale.max()], include_lowest=True, labels=[\"low\", \"medium\", \"high\"])\n",
    "spending_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average cost of a basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_basket_groupby_sum = df.groupby([\"CustomerID\",'BasketID'], as_index=False).agg({\"Sale\":sum})\n",
    "\n",
    "series_customer_basket_groupby_mean = df_customer_basket_groupby_sum.groupby('CustomerID')['Sale'].mean()\n",
    "df_customer_basket_groupby_mean = pd.DataFrame(df_customer_basket_groupby_mean)\n",
    "\n",
    "binwidth = 50\n",
    "bins=range(0, 400 + binwidth, binwidth)\n",
    "print(bins)\n",
    "n, bins, patches = plt.hist(df_customer_basket_groupby_mean.Sale, bins=bins, facecolor='blue', alpha=0.5)\n",
    "plt.savefig('../output/total_receipt_price_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "basket_cost_profile = pd.cut(df_customer_basket_groupby_mean['Sale'], bins=[0, 50, 200, df_customer_basket_groupby_mean.Sale.max()], include_lowest=True, labels=[\"low\", \"medium\", \"high\"])\n",
    "basket_cost_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales per country\n",
    "(is this even an indicator?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_per_country = df.groupby([\"CustomerCountry\"])[\"Sale\"].sum().reset_index()\n",
    "print(df_sales_per_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pie plot is horrible because of england, that probably has some outliers\n",
    "#explode=np.zeros(len(df_sales_per_country.CustomerCountry))\n",
    "\n",
    "plt.pie(df_sales_per_country.Sale, labels=df_sales_per_country.CustomerCountry, autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most bought items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_dropped = [2, 3, 6]\n",
    "df_most_bought = df.groupby([\"ProdID\", \"ProdDescr\"]).sum().reset_index()\n",
    "#df_most_bought = df_most_bought.drop(df_most_bought.columns[cols_dropped], axis=1)\n",
    "df_most_bought = df_most_bought.sort_values(by=\"Qta\", ascending=False)\n",
    "#df_most_bought = df_most_bought.groupby(level=0).head(1).reset_index()\n",
    "df_most_bought \n",
    "\n",
    "#print(df[df.ProdDescr == \"Discount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df_most_bought[:10].Qta, labels=df_most_bought[:10].ProdDescr, autopct='%1.1f%%')\n",
    "plt.savefig(\"../output/most_bought_item_piechart.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most bought item per country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols_dropped = [3, 4, 7]\n",
    "df_mb_country = df.groupby([\"ProdID\", \"ProdDescr\", \"CustomerCountry\"]).sum().reset_index()\n",
    "df_mb_country = df_mb_country.drop([\"Sale\"], axis = 1)\n",
    "\n",
    "df_mb_country = df_mb_country[df_mb_country.groupby([\"CustomerCountry\"])[\"Qta\"].transform(\"max\") == df_mb_country[\"Qta\"]].reset_index()\n",
    "\n",
    "#df_mb_country = df_mb_country.groupby([\"CustomerCountry\"]).agg({\"Qta\" : \"max\"}).reset_index()\n",
    "#print(df_mb_country.CustomerCountry.unique())\n",
    "\n",
    "\n",
    "print(df_mb_country)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end datetimes\n",
    "print(df.BasketDate.min(), df.BasketDate.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly = df.set_index(\"BasketDate\").copy()\n",
    "weekly = weekly.groupby(pd.Grouper(freq='M'))[\"Qta\"].sum()\n",
    "\n",
    "#print(weekly)\n",
    "\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.plot(weekly.index, weekly, color='tab:blue', marker=\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
