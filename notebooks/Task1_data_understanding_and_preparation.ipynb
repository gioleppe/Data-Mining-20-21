{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import iplot\n",
    "\n",
    "CUSTOMER_SUPERMARKET = \"../dataset/customer_supermarket.csv\"\n",
    "SALE_QTA_MERGE = \"../dataset/customer_supermarket_sale_qta_merge.csv\"\n",
    "INCONSISTENT_PROD = '../dataset/inconsistent_ProdID_ProdDescr.json'\n",
    "INCONSISTENT_COUNTRY = '../dataset/inconsistent_CustomerID_CustomerCountry.json'\n",
    "FINAL_UNDERSTANDING = \"../dataset/customer_supermarket_understanding.csv\"\n",
    "\n",
    "NO_INCONSISTENCY = \"../dataset/customer_supermarket_no_inconsistency.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CUSTOMER_SUPERMARKET, sep=\"\\t\", index_col=0, parse_dates=[\"BasketDate\"], decimal=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can have an idea of data types we are going to use\n",
    "\n",
    "| Attribute       | Type       |\n",
    "|-----------------|------------|\n",
    "| BasketID        | int64      |\n",
    "| BasketDate      | datetime64 |\n",
    "| Sale            | float64    |\n",
    "| CustomerID      | int64      |\n",
    "| CustomerCountry | object     |\n",
    "| ProdID          | object     |\n",
    "| ProdDescr       | object     |\n",
    "| Qta             | int64      |\n",
    "\n",
    "as we can see there are problems in the attribute values, so we have to evaluate what prevents us from getting the desired data types:\n",
    "\n",
    "The execution df['BasketID'].astype(int) tell us that some rows contains literal inside the attribute BasketID, this means the data type cannot be converted to int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking NaN values and duplicates\n",
    "\n",
    "It is possible to evaluate the quality of the data from the point of view of the rows, based on two aspects:\n",
    "- missing or partial value (NaN/Null values)\n",
    "- duplicates\n",
    "- duplicates products in the same basket\n",
    "\n",
    "Any duplicates or rows with missing values are temporarily removed from the dataset to allow for better evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are duplicates\n",
    "areduplicates = df.duplicated().any()\n",
    "\n",
    "# duplicates have been removed from the data set\n",
    "df_nodup = df.drop_duplicates()\n",
    "\n",
    "# check if there are missing or incomplete values\n",
    "arenan = df.isnull().values.any()\n",
    "\n",
    "# rows with missing or incomplete values have been removed from the data set\n",
    "df_clean = df_nodup.dropna()\n",
    "\n",
    "print(\"Duplicate rows in the dataset: \", areduplicates, \"| Number or rows removed: \", len(df)-len(df_nodup))\n",
    "print(\"NaN/Null values in the dataset: \", arenan, \"| Number or rows removed: \", len(df_nodup)-len(df_clean))\n",
    "\n",
    "df_Sale_Qta_merge = df_clean.groupby(['BasketID','BasketDate','ProdID']).agg({'Qta':np.sum,\n",
    "                                                                         'Sale':np.sum,\n",
    "                                                                         'CustomerID':'min',\n",
    "                                                                         'CustomerCountry':'min',\n",
    "                                                                         'ProdDescr':'min'}).reset_index()\n",
    "\n",
    "df_Sale_Qta_merge.to_csv(SALE_QTA_MERGE, sep=\"\\t\", decimal=\",\")\n",
    "\n",
    "print(f\"Number of identical items in multiple rows of the same receipt {len(df_clean) - len(df_Sale_Qta_merge)}\")\n",
    "\n",
    "df = df_Sale_Qta_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking data consistency\n",
    "\n",
    "Here we are verifing that the dataset is consistent:\n",
    "\n",
    "1. Every ProdID must match the same ProdDescr\n",
    "2. Every CustomerID must match the same BasketID in the same BasketDate\n",
    "3. Every CustomerID must match the same CustomerCountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inconsistent_set(K,V):\n",
    "    \n",
    "    inconsistentset = list()\n",
    "\n",
    "    for key in tqdm(df[K].unique().tolist()):\n",
    "        temp_df = df[df[K] == key]\n",
    "        valueslist = temp_df[V].tolist()\n",
    "        for value in valueslist:\n",
    "            if(valueslist[0] != value):\n",
    "                inconsistentset.append(key)\n",
    "                break;\n",
    "    return inconsistentset\n",
    "\n",
    "\n",
    "# 1\n",
    "ProdID_ProdDescr_IS = inconsistent_set(\"ProdID\",\"ProdDescr\")\n",
    "            \n",
    "print(\"Number of not consistent ProdDescr:\", len(ProdID_ProdDescr_IS))\n",
    "\n",
    "with open(INCONSISTENT_PROD, 'w') as f:\n",
    "    json.dump(ProdID_ProdDescr_IS, f, sort_keys=True)\n",
    "\n",
    "\n",
    "#2 True == no error\n",
    "temp = df.groupby(['BasketID','BasketDate']).CustomerID.nunique().eq(1)\n",
    "print(\"Every CustomerID matches the same BasketID in the same BasketDate: \", temp.all())\n",
    "\n",
    "#to avoid graphical printing bug\n",
    "time.sleep(0.1)\n",
    "#3 \n",
    "CustomerID_CustomerCountry_IS = inconsistent_set(\"CustomerID\",\"CustomerCountry\")\n",
    "            \n",
    "print(\"Number of not consistent CustomerCountry:\", len(CustomerID_CustomerCountry_IS))\n",
    "\n",
    "\n",
    "with open(INCONSISTENT_COUNTRY, 'w') as f:\n",
    "    json.dump(CustomerID_CustomerCountry_IS, f, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting attributes to correct data type\n",
    "\n",
    "Here we are changing the type of attributes. This imply a little cleaning phase over BasketID because there are characters inside the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(SALE_QTA_MERGE, sep=\"\\t\", index_col=0, parse_dates=[\"BasketDate\"], decimal=\",\")\n",
    "\n",
    "df['CustomerID'] = pd.to_numeric(df.CustomerID)\n",
    "df.CustomerID = df.CustomerID.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics and data understanding\n",
    "\n",
    "Here are informations about quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of clients: \",len(df['CustomerID'].unique()))\n",
    "print(\"Number of products: \",len(df['ProdID'].unique()))\n",
    "print(\"Number of purchase: \",len(df['BasketID'].unique()))\n",
    "print(\"Distinct Customer Country: \\t\", len(df.CustomerCountry.unique()))\n",
    "\n",
    "# number of items cancelled in the basket by every customer\n",
    "basket_cancelled = df[df['BasketID'].str.contains(\"C\")]\n",
    "print(\"Number or rows with cancelled items: \", len(basket_cancelled))\n",
    "\n",
    "#print(\"Distinct values in ProdID: \\t\", df_clean.ProdID.unique())\n",
    "#print(\"Distinct values in ProdDescr: \\t\", df_clean.ProdDescr.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have people coming from unspecified countries, a generic \"European Community\", EIRE which stands for Ireland and RSA which stands for Republic of South Africa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of customers from Unspecified countries: \\t\", (df.CustomerCountry == \"Unspecified\").sum())\n",
    "print(\"Number of customers from generic European Community countries: \\t\", (df.CustomerCountry == \"European Community\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be absolutely no correlation between the values of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = df.corr(method=\"pearson\")\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "mask = np.triu(np.ones_like(correlations, dtype=bool))\n",
    "\n",
    "# this diverging palette is pretty dull since everything is unrelated\n",
    "#cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "cmap = sns.color_palette(\"Blues\")\n",
    "sns.heatmap(correlations, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "            annot=True, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = df.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df[['BasketID', 'CustomerID', 'Qta', 'Sale']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qta_std = df['Qta'].std()\n",
    "qta_mean = df['Qta'].mean()\n",
    "\n",
    "sale_std = df['Sale'].std()\n",
    "sale_mean = df['Sale'].mean()\n",
    "\n",
    "print(\"QTA STD: \", qta_std, \" QTA MEAN: \", qta_mean)\n",
    "print(\"SALE STD: \", sale_std, \" SALE MEAN: \", sale_mean)\n",
    "\n",
    "threshold = 3\n",
    "\n",
    "def outliers_zscore(qta,sale):\n",
    "    \n",
    "    qta_z_score = (qta - qta_mean) / qta_std\n",
    "    if(np.abs(qta_z_score) > threshold):\n",
    "        return True\n",
    "    \n",
    "    sale_z_score = (sale - sale_mean) / sale_std\n",
    "    if(np.abs(sale_z_score) > threshold):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "    \n",
    "df['Outlier'] = df.apply(lambda x: outliers_zscore(x['Qta'],x['Sale']), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_df = df[df['Outlier'] == True]\n",
    "notoutliers_df = df[df['Outlier'] == False]\n",
    "\n",
    "plt.scatter(outliers_df['Qta'],outliers_df['Sale'], color='r', marker='*', label=\"outliers\")\n",
    "\n",
    "plt.scatter(notoutliers_df['Qta'],notoutliers_df['Sale'], color='g', marker='*', label=\"normal data\")\n",
    "\n",
    "\n",
    "plt.xlabel('Qta')\n",
    "plt.ylabel('Sale')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(FINAL_UNDERSTANDING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "The following cells will improve the DF, which presents inconsistency, missing values and outliers, thanks to consideration done during the data understanding phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df\n",
    "df = pd.read_csv(FINAL_UNDERSTANDING, index_col=0, parse_dates=[\"BasketDate\"], decimal=\",\")\n",
    "df.Sale = df.Sale.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every canceled basket ('C'+'BasketID') check if exists at least one counterpart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = df[(df['BasketID'].str.contains('C')) & (df['ProdID'] != 'D')][['CustomerID','Qta','ProdID']]\n",
    "for index, col in  df_check.iterrows():\n",
    "    if df[(df['CustomerID'] == col[0]) & (df['Qta'] == -col[1]) & (df['ProdID'] == col[2])].shape[0] == 0: \n",
    "        print(True)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove from the dataset the rows with canceled basket and possible counterpart (if there are more then one counterpart, the first will be deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df[(df['BasketID'].str.contains('C')) & (df['ProdID'] != 'D')]\n",
    "\n",
    "rows_with_counterparts = []\n",
    "rows_without_counterparts = []\n",
    "\n",
    "for index, col in df_temp.iterrows():\n",
    "    df_temp = df[(df['CustomerID'] == col['CustomerID']) & (df['Qta'] == -col['Qta']) & (df['ProdID'] == col['ProdID'])]\n",
    "    \n",
    "    if df_temp.shape[0] == 0: \n",
    "        rows_without_counterparts.append(index)\n",
    "    else:\n",
    "        rows_with_counterparts.append(index)\n",
    "        rows_with_counterparts.append(df_temp.index[0])\n",
    "    \n",
    "rows_to_be_dropped = rows_with_counterparts + rows_without_counterparts\n",
    "\n",
    "df_canceled_basket = df.drop(df.index[rows_to_be_dropped])\n",
    "\n",
    "print(\"Total number of dropped rows: \", len(df)-len(df_canceled_basket))\n",
    "\n",
    "df = df_canceled_basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inconsistency resolution\n",
    "\n",
    "def inconsistency_resolver(path,col1,col2):\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        inconsistent_list = json.load(f)\n",
    "        \n",
    "    df_inconsistent = df[df[col1].isin(inconsistent_list)]\n",
    "\n",
    "    df_grouped = df_inconsistent.groupby([col1,col2]).size().reset_index()\n",
    "    \n",
    "    df_grouped = df_grouped.sort_values(0, ascending=False).drop_duplicates(col1).sort_index()\n",
    "    \n",
    "    mydict = pd.Series(df_grouped[col2].values,index=df_grouped[col1]).to_dict()\n",
    "    \n",
    "    for k,v in mydict.items():\n",
    "        \n",
    "        df.loc[df[col1] == k, col2] = v\n",
    "\n",
    "inconsistency_resolver(INCONSISTENT_COUNTRY,\"CustomerID\",\"CustomerCountry\") \n",
    "inconsistency_resolver(INCONSISTENT_PROD,\"ProdID\",\"ProdDescr\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' check if inconsistency has been solved\n",
    "# 1\n",
    "ProdID_ProdDescr_IS = inconsistent_set(\"ProdID\",\"ProdDescr\")\n",
    "            \n",
    "print(\"Number of not consistent ProdDescr:\", len(ProdID_ProdDescr_IS))\n",
    "\n",
    "#3 \n",
    "CustomerID_CustomerCountry_IS = inconsistent_set(\"CustomerID\",\"CustomerCountry\")\n",
    "            \n",
    "print(\"Number of not consistent CustomerCountry:\", len(CustomerID_CustomerCountry_IS))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF without inconsistency serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(NO_INCONSISTENCY, sep=\"\\t\", decimal=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first of all we deserialize our dataframe\n",
    "df = pd.read_csv(NO_INCONSISTENCY, sep=\"\\t\", index_col=0, parse_dates=[\"BasketDate\"], decimal=\",\")\n",
    "# second remove outliers from df\n",
    "df = df[df['Outlier'] == False]\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - indicator\n",
    "the total number of items purchased by a customer during the period of\n",
    "observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = df.groupby(\"CustomerID\").sum().reset_index()\n",
    "df_i = df_i[[\"CustomerID\", \"Qta\"]]\n",
    "df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i.sort_values(by='Qta', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iu - indicator\n",
    "the number of distinct items bought by a customer in the period of\n",
    "observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iu = df.groupby('CustomerID')['ProdID'].nunique().reset_index()\n",
    "df_iu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imax - indicator\n",
    "the maximum number of items purchased by a customer during a\n",
    "shopping session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imax = df.groupby([\"CustomerID\", \"BasketID\"]).Qta.sum()\n",
    "df_imax = df_imax.groupby(level=0).head(1).reset_index()\n",
    "\n",
    "#df_imax = df_imax.max(level=0)\n",
    "\n",
    "df_imax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E - indicator\n",
    "the Shannon entropy on the purchasing behaviour of the customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy helper function \n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from math import log, e\n",
    "import pandas as pd   \n",
    "\n",
    "\"\"\" Usage: pandas_entropy(df['column1']) \"\"\"\n",
    "\n",
    "def pandas_entropy(column, base=None):\n",
    "    vc = pd.Series(column).value_counts(normalize=True, sort=False)\n",
    "    base = e if base is None else base\n",
    "    return -(vc * np.log(vc)/np.log(base)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.groupby([\"CustomerID\", \"BasketID\"]).Qta.sum().reset_index()\n",
    "df_entropy = pd.DataFrame(df.CustomerID.unique(), columns=[\"CustomerID\"])\n",
    "\n",
    "entropies = []\n",
    "\n",
    "for customer in df_temp.CustomerID.unique():\n",
    "    customer_baskets = df_temp[df_temp.CustomerID == customer]\n",
    "    entropies.append(pandas_entropy(customer_baskets[\"Qta\"]))\n",
    "    #print(df_temp[df_temp.CustomerID == customer])\n",
    "    \n",
    "df_entropy[\"Entropy\"] = entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Spending\n",
    "We compute the total amount spent by each customer in the observation period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tot = df.groupby([\"CustomerID\"]).Sale.sum()\n",
    "df_tot = df_tot.groupby(level=0).head(1).reset_index()\n",
    "\n",
    "print(df_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average spending \n",
    "We compute the average basket value for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_basket_groupby_sum = df.groupby([\"CustomerID\",'BasketID'], as_index=False).agg({\"Sale\":sum})\n",
    "df_mean = df_customer_basket_groupby_sum.groupby('CustomerID')['Sale'].mean()\n",
    "df_mean = df_mean.groupby(level=0).head(1).reset_index()\n",
    "\n",
    "print(df_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounts per user\n",
    "We count the number of times a customer used a discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_discounts = df.where(df.ProdDescr == \"Discount\").groupby([\"CustomerID\"]).count()\n",
    "print(df_discounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting together all indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_tot.Sale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indicators = pd.DataFrame(df.CustomerID.unique(), columns=[\"CustomerID\"]).sort_values(by=['CustomerID'])\n",
    "df_indicators[\"Entropy\"] = df_entropy.Entropy.values\n",
    "df_indicators[\"i\"] = df_i.Qta.values\n",
    "df_indicators[\"iu\"] = df_iu.ProdID.values\n",
    "df_indicators[\"imax\"] = df_imax.Qta.values\n",
    "df_indicators[\"tot_sales\"] = df_tot.Sale.values\n",
    "df_indicators[\"mean_sales\"] = df_mean.Sale.values\n",
    "\n",
    "indicators.to_csv(\"../dataset/indicators.csv\")\n",
    "print(df_indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_indicators.discounts_used.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df.groupby('CustomerCountry').CustomerID.nunique().reset_index()\n",
    "temp = temp.sort_values(by=['CustomerID'], ascending=True)\n",
    "\n",
    "customer_list = temp['CustomerID']\n",
    "country_list = temp['CustomerCountry']\n",
    "\n",
    "fig = go.Figure(go.Bar(\n",
    "            x=xs,\n",
    "            y=xy,\n",
    "            orientation='h'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid of plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = px.data.tips()\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\"Plot 1\", \"Plot 2\", \"Plot 3\", \"Plot 4\")\n",
    ")\n",
    "\n",
    "bins = int(np.log(len(df_tot))+1)\n",
    "\n",
    "#fig = px.histogram(df_tot, x=\"Sale\")\n",
    "\n",
    "# customer country distribution\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=xs,\n",
    "        y=xy,\n",
    "        orientation='h'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# amount spent by customer distribution \n",
    "fig.add_trace(\n",
    "    go.Histogram(x=df_tot['Sale'].tolist()),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# amount spent by customer distribution \n",
    "fig.add_trace(\n",
    "    go.Histogram(x=df_i['Qta'].tolist()),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(height=1000, width=1200, title_text=\"Side By Side Subplots\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[['CustomerID', 'CustomerCountry']].groupby(['CustomerID', 'CustomerCountry']).count()\n",
    "temp = temp.reset_index(drop = False)\n",
    "countries = temp['CustomerCountry'].value_counts()\n",
    "data = dict(type='choropleth',\n",
    "    locations = countries.index,\n",
    "    locationmode = 'country names', z = countries,\n",
    "    text = countries.index, colorbar = {'title':'Order nb.'},\n",
    "    colorscale=[[0, 'rgb(224,255,255)'],\n",
    "                [0.01, 'rgb(166,206,227)'], [0.02, 'rgb(31,120,180)'],\n",
    "                [0.03, 'rgb(178,223,138)'], [0.05, 'rgb(51,160,44)'],\n",
    "                [0.10, 'rgb(251,154,153)'], [0.20, 'rgb(255,255,0)'],\n",
    "                [1, 'rgb(227,26,28)']],    \n",
    "    reversescale = False)\n",
    "#_______________________\n",
    "layout = dict(title='Number of orders per country',height=1000, width=1200,\n",
    "    geo = dict(showframe = True, projection={'type':'mercator'})\n",
    "            )\n",
    "#______________\n",
    "choromap = go.Figure(data = [data], layout = layout)\n",
    "iplot(choromap, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer Spending Profile \n",
    "we categorize each customer as either low, medium, or high spending according to their average expense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profile = df.groupby([\"CustomerID\"]).agg({\"Sale\":sum, \"Qta\":sum})\n",
    "\n",
    "binwidth = 50\n",
    "bins=range(0, 1000 + binwidth, binwidth)\n",
    "print(bins)\n",
    "n, bins, patches = plt.hist(df_profile.Sale, bins=bins, facecolor='blue', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "spending_profile = pd.cut(df_profile['Sale'], bins=[0, 100, 300, df_profile.Sale.max()], include_lowest=True, labels=[\"low\", \"medium\", \"high\"])\n",
    "spending_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average cost of a basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_basket_groupby_sum = df.groupby([\"CustomerID\",'BasketID'], as_index=False).agg({\"Sale\":sum})\n",
    "\n",
    "series_customer_basket_groupby_mean = df_customer_basket_groupby_sum.groupby('CustomerID')['Sale'].mean()\n",
    "df_customer_basket_groupby_mean = pd.DataFrame(df_customer_basket_groupby_mean)\n",
    "\n",
    "binwidth = 50\n",
    "bins=range(0, 400 + binwidth, binwidth)\n",
    "print(bins)\n",
    "n, bins, patches = plt.hist(df_customer_basket_groupby_mean.Sale, bins=bins, facecolor='blue', alpha=0.5)\n",
    "plt.savefig('../output/total_receipt_price_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "basket_cost_profile = pd.cut(df_customer_basket_groupby_mean['Sale'], bins=[0, 50, 200, df_customer_basket_groupby_mean.Sale.max()], include_lowest=True, labels=[\"low\", \"medium\", \"high\"])\n",
    "basket_cost_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_per_country = df.groupby([\"CustomerCountry\"])[\"Sale\"].sum().reset_index()\n",
    "print(df_sales_per_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pie plot is horrible because of england, that probably has some outliers\n",
    "#explode=np.zeros(len(df_sales_per_country.CustomerCountry))\n",
    "\n",
    "plt.pie(df_sales_per_country.Sale, labels=df_sales_per_country.CustomerCountry, autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most bought items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_dropped = [2, 3, 6]\n",
    "df_most_bought = df.groupby([\"ProdID\", \"ProdDescr\"]).sum().reset_index()\n",
    "#df_most_bought = df_most_bought.drop(df_most_bought.columns[cols_dropped], axis=1)\n",
    "df_most_bought = df_most_bought.sort_values(by=\"Qta\", ascending=False)\n",
    "#df_most_bought = df_most_bought.groupby(level=0).head(1).reset_index()\n",
    "df_most_bought \n",
    "\n",
    "#print(df[df.ProdDescr == \"Discount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df_most_bought[:10].Qta, labels=df_most_bought[:10].ProdDescr, autopct='%1.1f%%')\n",
    "plt.savefig(\"../output/most_bought_item_piechart.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most bought item per country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols_dropped = [3, 4, 7]\n",
    "df_mb_country = df.groupby([\"ProdID\", \"ProdDescr\", \"CustomerCountry\"]).sum().reset_index()\n",
    "df_mb_country = df_mb_country.drop([\"Sale\"], axis = 1)\n",
    "\n",
    "df_mb_country = df_mb_country[df_mb_country.groupby([\"CustomerCountry\"])[\"Qta\"].transform(\"max\") == df_mb_country[\"Qta\"]].reset_index()\n",
    "\n",
    "#df_mb_country = df_mb_country.groupby([\"CustomerCountry\"]).agg({\"Qta\" : \"max\"}).reset_index()\n",
    "#print(df_mb_country.CustomerCountry.unique())\n",
    "\n",
    "\n",
    "print(df_mb_country)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end datetimes\n",
    "print(df.BasketDate.min(), df.BasketDate.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly = df.set_index(\"BasketDate\").copy()\n",
    "weekly = weekly.groupby(pd.Grouper(freq='M'))[\"Qta\"].sum()\n",
    "\n",
    "#print(weekly)\n",
    "\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.plot(weekly.index, weekly, color='tab:blue', marker=\"o\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
